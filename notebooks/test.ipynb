{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "data_dir = os.path.join('..', \"outputs\", \"embeddings\")\n",
    "with open(os.path.join(data_dir, \"loaders_datasets.pkl\"), 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train audio features shape: torch.Size([9988, 768])\n",
      "train text features shape: torch.Size([9988, 768])\n",
      "train video features shape: torch.Size([9988, 16, 768])\n",
      "train text length tensor shape: torch.Size([9988])\n",
      "val audio features shape: torch.Size([1108, 768])\n",
      "val text features shape: torch.Size([1108, 768])\n",
      "val video features shape: torch.Size([1108, 16, 768])\n",
      "val text length tensor shape: torch.Size([1108])\n",
      "test audio features shape: torch.Size([2610, 768])\n",
      "test text features shape: torch.Size([2610, 768])\n",
      "test video features shape: torch.Size([2610, 16, 768])\n",
      "test text length tensor shape: torch.Size([2610])\n"
     ]
    }
   ],
   "source": [
    "for split in data.keys():\n",
    "    for modal in ['audio', 'text', 'video']:\n",
    "        modal_tensors = data[split][modal].tensors\n",
    "        data[split][modal] = {\n",
    "            'features': modal_tensors[0], \n",
    "            'labels': modal_tensors[1]   \n",
    "        }\n",
    "\n",
    "    if 'text' in data[split]:\n",
    "        text_features = data[split]['text']['features']\n",
    "        text_len_tensor = torch.sum((text_features != 0).long(), dim=1) \n",
    "        data[split]['text']['text_len_tensor'] = text_len_tensor\n",
    "\n",
    "\n",
    "for split in data.keys():\n",
    "    print(f\"{split} audio features shape: {data[split]['audio']['features'].shape}\")\n",
    "    print(f\"{split} text features shape: {data[split]['text']['features'].shape}\")\n",
    "    print(f\"{split} video features shape: {data[split]['video']['features'].shape}\")\n",
    "    print(f\"{split} text length tensor shape: {data[split]['text']['text_len_tensor'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, RGCNConv\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Préparer les données --------\n",
    "def prepare_graph_data(data, split):\n",
    "    # Moyennage sur la dimension temporelle pour la modalité vidéo\n",
    "    video_features = torch.mean(data[split]['video']['features'], dim=1)  # Moyenne sur la dimension temporelle\n",
    "    \n",
    "    # Concaténer les features\n",
    "    features = torch.cat([\n",
    "        data[split]['audio']['features'],\n",
    "        data[split]['text']['features'],\n",
    "        video_features\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Concaténer les labels\n",
    "    labels = torch.cat([\n",
    "        data[split]['audio']['labels'],\n",
    "        data[split]['text']['labels'],\n",
    "        data[split]['video']['labels']\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Initialiser les listes pour edge_index et edge_type\n",
    "    edge_index = []\n",
    "    edge_type = []\n",
    "\n",
    "    # -------- Relations temporelles intra-modales --------\n",
    "    def add_temporal_edges(offset, num_nodes, relation_type):\n",
    "        for i in range(num_nodes):\n",
    "            if i > 0:\n",
    "                edge_index.append([offset + i, offset + i - 1])  # -1 (passé)\n",
    "                edge_type.append(relation_type)\n",
    "            edge_index.append([offset + i, offset + i])  # 0 (présent)\n",
    "            edge_type.append(relation_type + 1)\n",
    "            if i < num_nodes - 1:\n",
    "                edge_index.append([offset + i, offset + i + 1])  # +1 (futur)\n",
    "                edge_type.append(relation_type + 2)\n",
    "\n",
    "    # Ajouter les relations intra-modales\n",
    "    num_audio = data[split]['audio']['features'].size(0)\n",
    "    num_text = data[split]['text']['features'].size(0)\n",
    "    num_video = video_features.size(0)  # Taille après réduction temporelle\n",
    "\n",
    "    add_temporal_edges(0, num_audio, 0)  # Audio : relation types 0, 1, 2\n",
    "    add_temporal_edges(num_audio, num_text, 3)  # Text : relation types 3, 4, 5\n",
    "    add_temporal_edges(num_audio + num_text, num_video, 6)  # Video : relation types 6, 7, 8\n",
    "\n",
    "    # -------- Relations cross-modales --------\n",
    "    for i in range(min(num_audio, num_text, num_video)):\n",
    "        edge_index.append([i, num_audio + num_text + i])  # Audio -> Video\n",
    "        edge_type.append(9)\n",
    "        edge_index.append([num_audio + i, num_audio + num_text + i])  # Text -> Video\n",
    "        edge_type.append(10)\n",
    "        edge_index.append([i, num_audio + i])  # Audio -> Text\n",
    "        edge_type.append(11)\n",
    "\n",
    "    # Convertir en tenseurs\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "\n",
    "    print(\"Unique edge types:\", torch.unique(edge_type))  # Debugging\n",
    "    return Data(x=features, y=labels, edge_index=edge_index, edge_type=edge_type)\n",
    "\n",
    "# -------- Définir les Modèles --------\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.0):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations)\n",
    "        self.conv2 = RGCNConv(hidden_channels, out_channels, num_relations)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# -------- Entraîner et Tester --------\n",
    "def train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(model, RGCN):\n",
    "            out = model(train_data.x, train_data.edge_index, train_data.edge_type)\n",
    "        \n",
    "        else:\n",
    "            out = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "        loss = criterion(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, RGCN):\n",
    "                val_out = model(val_data.x, val_data.edge_index, val_data.edge_type)\n",
    "            else:\n",
    "                val_out = model(val_data.x, val_data.edge_index)\n",
    "            val_loss = criterion(val_out, val_data.y)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, RGCN):\n",
    "            test_out = model(test_data.x, test_data.edge_index, test_data.edge_type)\n",
    "        else:\n",
    "            test_out = model(test_data.x, test_data.edge_index)\n",
    "        test_pred = test_out.argmax(dim=1)\n",
    "        test_acc = accuracy_score(test_data.y.cpu(), test_pred.cpu())\n",
    "        test_f1 = f1_score(test_data.y.cpu(), test_pred.cpu(), average='macro')\n",
    "\n",
    "    return test_acc, test_f1\n",
    "\n",
    "# -------- Pipeline Principal --------\n",
    "def main_pipeline(data):\n",
    "    # Préparer les données\n",
    "    train_data = prepare_graph_data(data, 'train')\n",
    "    val_data = prepare_graph_data(data, 'val')\n",
    "    test_data = prepare_graph_data(data, 'test')\n",
    "\n",
    "    # Configurations de modèles\n",
    "    architectures = [\n",
    "        (\"GCN (2 layers)\", GCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)))),\n",
    "        (\"GAT (4 heads)\", GAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), heads=4)),\n",
    "        (\"RGCN\", RGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=12)),\n",
    "    ]\n",
    "\n",
    "    # Entraîner et évaluer chaque modèle\n",
    "    results = []\n",
    "    for name, model in architectures:\n",
    "        print(f\"Training {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        acc, f1 = train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion)\n",
    "        results.append((name, acc * 100, f1 * 100))\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"Model\\t\\t\\tTest Accuracy\\tTest F1-Score\")\n",
    "    for name, acc, f1 in results:\n",
    "        print(f\"{name:20}\\t{acc:.2f}\\t\\t{f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Improvement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Normalisation des Données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_graph_data(data, split, graph_augmentation=False):\n",
    "    # Normaliser les données\n",
    "    scaler_audio = StandardScaler()\n",
    "    scaler_text = StandardScaler()\n",
    "    scaler_video = StandardScaler()\n",
    "\n",
    "    data[split]['audio']['features'] = torch.tensor(\n",
    "        scaler_audio.fit_transform(data[split]['audio']['features']),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    data[split]['text']['features'] = torch.tensor(\n",
    "        scaler_text.fit_transform(data[split]['text']['features']),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    video_shape = data[split]['video']['features'].shape\n",
    "    data[split]['video']['features'] = torch.tensor(\n",
    "        scaler_video.fit_transform(data[split]['video']['features'].reshape(-1, video_shape[-1])),\n",
    "        dtype=torch.float\n",
    "    ).reshape(video_shape)\n",
    "\n",
    "    # Moyenne sur la dimension temporelle pour la modalité vidéo\n",
    "    video_features = torch.mean(data[split]['video']['features'], dim=1)  # Moyenne sur la dimension temporelle\n",
    "    \n",
    "    # Concaténer les features\n",
    "    features = torch.cat([\n",
    "        data[split]['audio']['features'],\n",
    "        data[split]['text']['features'],\n",
    "        video_features\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Concaténer les labels\n",
    "    labels = torch.cat([\n",
    "        data[split]['audio']['labels'],\n",
    "        data[split]['text']['labels'],\n",
    "        data[split]['video']['labels']\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Initialiser les listes pour edge_index et edge_type\n",
    "    edge_index = []\n",
    "    edge_type = []\n",
    "\n",
    "    # -------- Relations temporelles intra-modales --------\n",
    "    def add_temporal_edges(offset, num_nodes, relation_type):\n",
    "        for i in range(num_nodes):\n",
    "            if i > 1:\n",
    "                edge_index.append([offset + i, offset + i - 2])  # Passé lointain\n",
    "                edge_type.append(relation_type + 3)\n",
    "            if i < num_nodes - 2:\n",
    "                edge_index.append([offset + i, offset + i + 2])  # Futur lointain\n",
    "                edge_type.append(relation_type + 4)\n",
    "\n",
    "            # Passé proche, présent, futur proche\n",
    "            if i > 0:\n",
    "                edge_index.append([offset + i, offset + i - 1])\n",
    "                edge_type.append(relation_type)\n",
    "            edge_index.append([offset + i, offset + i])\n",
    "            edge_type.append(relation_type + 1)\n",
    "            if i < num_nodes - 1:\n",
    "                edge_index.append([offset + i, offset + i + 1])\n",
    "                edge_type.append(relation_type + 2)\n",
    "\n",
    "\n",
    "    # Ajouter les relations intra-modales\n",
    "    num_audio = data[split]['audio']['features'].size(0)\n",
    "    num_text = data[split]['text']['features'].size(0)\n",
    "    num_video = video_features.size(0)  # Taille après réduction temporelle\n",
    "\n",
    "    add_temporal_edges(0, num_audio, 0)  # Audio\n",
    "    add_temporal_edges(num_audio, num_text, 5)  # Text\n",
    "    add_temporal_edges(num_audio + num_text, num_video, 10)  # Video\n",
    "\n",
    "    # Ajouter les relations cross-modales complexes\n",
    "    for i in range(min(num_audio, num_text, num_video)):\n",
    "        edge_index.append([i, num_audio + num_text + i])  # Audio -> Video\n",
    "        edge_type.append(15)\n",
    "        edge_index.append([num_audio + i, num_audio + num_text + i])  # Text -> Video\n",
    "        edge_type.append(16)\n",
    "        edge_index.append([i, num_audio + i])  # Audio -> Text\n",
    "        edge_type.append(17)\n",
    "\n",
    "    # Graph augmentation : ajouter des arêtes aléatoires si activé\n",
    "    if graph_augmentation:\n",
    "        for _ in range(100):  # Par exemple, 100 arêtes aléatoires\n",
    "            src, dst = torch.randint(0, features.size(0), (2,))\n",
    "            edge_index.append([src.item(), dst.item()])\n",
    "            edge_type.append(18)\n",
    "\n",
    "    # Convertir en tenseurs\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "\n",
    "    print(\"Unique edge types:\", torch.unique(edge_type))  # Debugging\n",
    "    return Data(x=features, y=labels, edge_index=edge_index, edge_type=edge_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Equilibrage des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "    weights = 1.0 / counts.float()\n",
    "    weights = weights / weights.sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Augmentation des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_augmentation = True\n",
    "# train_data = prepare_graph_data(data, 'train', graph_augmentation=graph_augmentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Architecture des modèles** (Ajout de Couches Supplémentaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.3):\n",
    "        super(DeepGCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.final_layer = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            x = layer(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "class DeepGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, num_layers=3, dropout=0.5):\n",
    "        super(DeepGAT, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GATConv(in_channels, hidden_channels, heads=heads))\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels * heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels * heads))\n",
    "\n",
    "        self.final_layer = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            x = layer(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "class CombinedGCN_GAT_RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations, dropout=0.5):\n",
    "        super(CombinedGCN_GAT_RGCN, self).__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=4)\n",
    "        self.rgcn = RGCNConv(hidden_channels * 4, hidden_channels, num_relations)\n",
    "        self.final_layer = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = self.gcn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rgcn(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class TemporalGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations, heads=4, dropout=0.5):\n",
    "        super(TemporalGAT, self).__init__()\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        self.gat_layers.append(GATConv(in_channels, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        for _ in range(2):  # Exemple : 3 couches de GAT\n",
    "            self.gat_layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        self.final_layer = GATConv(hidden_channels * heads, out_channels, heads=1, add_self_loops=False)\n",
    "        self.num_relations = num_relations\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type=None):\n",
    "        if edge_type is None:\n",
    "            raise ValueError(\"edge_type is required for TemporalGAT\")\n",
    "\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)  \n",
    "            x = gat_layer(x, edge_index)  \n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class TemporalGATv2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.5):\n",
    "        super(TemporalGATv2, self).__init__()\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        self.gat_layers.append(GATv2Conv(in_channels, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        for _ in range(2):\n",
    "            self.gat_layers.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        self.final_layer = GATv2Conv(hidden_channels * heads, out_channels, heads=1, add_self_loops=False)\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)  # Rename to avoid conflict\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = gat_layer(x, edge_index)\n",
    "\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Relations dans le graphe\n",
    "\n",
    "a) Relations cross-modales\n",
    "\n",
    "Introduisez plus de connexions cross-modales dans edge_index, par exemple en liant chaque nœud audio à plusieurs nœuds vidéo proches au lieu d'un seul.\n",
    "\n",
    "b) Graph Augmentation\n",
    "\n",
    "Ajoutez des bruits aux connexions existantes ou utilisez des méthodes comme DropEdge (supprimer des arêtes aléatoires à chaque itération) pour améliorer la robustesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Autres Modèles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Graph Attention Networks (GATv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedGCN_GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(CombinedGCN_GAT, self).__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=4)\n",
    "        self.final_layer = GCNConv(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gcn(x, edge_index))\n",
    "        x = F.elu(self.gat(x, edge_index))\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion, epochs=50):\n",
    "    weights = compute_class_weights(train_data.y)\n",
    "    criterion = CrossEntropyLoss(weight=weights)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if hasattr(model, 'rgcn') or isinstance(model, (TemporalGAT, CombinedGCN_GAT_RGCN)):\n",
    "            out = model(train_data.x, train_data.edge_index, train_data.edge_type)\n",
    "        else:\n",
    "            out = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "        loss = criterion(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, (CombinedGCN_GAT_RGCN, TemporalGAT)):\n",
    "                val_out = model(val_data.x, val_data.edge_index, val_data.edge_type)\n",
    "            else:\n",
    "                val_out = model(val_data.x, val_data.edge_index)\n",
    "            val_loss = criterion(val_out, val_data.y)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, (TemporalGAT, CombinedGCN_GAT_RGCN)):\n",
    "            test_out = model(test_data.x, test_data.edge_index, test_data.edge_type)\n",
    "        else:\n",
    "            test_out = model(test_data.x, test_data.edge_index)\n",
    "        test_pred = test_out.argmax(dim=1)\n",
    "        test_acc = accuracy_score(test_data.y.cpu(), test_pred.cpu())\n",
    "        test_f1 = f1_score(test_data.y.cpu(), test_pred.cpu(), average='macro')\n",
    "\n",
    "    return test_acc, test_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data):\n",
    "    train_data = prepare_graph_data(data, 'train', graph_augmentation=False)\n",
    "    val_data = prepare_graph_data(data, 'val', graph_augmentation=False)\n",
    "    test_data = prepare_graph_data(data, 'test', graph_augmentation=False)\n",
    "\n",
    "    # Configurations de modèles\n",
    "    architectures = [\n",
    "        # (\"GCN (3 layers)\", DeepGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3)),\n",
    "        # (\"GCN (5 layers)\", DeepGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=5)),\n",
    "        # (\"GAT (3 layers)\", DeepGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3)),\n",
    "        # (\"GAT (5 layers)\", DeepGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=5)),\n",
    "        # (\"RGCN (5 relations)\", RGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=20)),\n",
    "        # (\"Temporal GAT\", TemporalGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=20)),\n",
    "        # (\"Temporal GATv2\", TemporalGATv2(train_data.x.size(1), 64, len(torch.unique(train_data.y)))),\n",
    "        # (\"Combined GCN+GAT\", CombinedGCN_GAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)))),\n",
    "        (\"GAT + BatchNorm\", DeepGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3)),\n",
    "        (\"GCN+GAT+RGCN\", CombinedGCN_GAT_RGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=20)),\n",
    "        (\"GCN + Dropout\", DeepGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3, dropout=0.5)),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, model in architectures:\n",
    "        print(f\"Training {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        acc, f1 = train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion)\n",
    "        results.append((name, acc * 100, f1 * 100))\n",
    "\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"Model\\t\\t\\tTest Accuracy\\tTest F1-Score\")\n",
    "    for name, acc, f1 in results:\n",
    "        print(f\"{name:20}\\t{acc:.2f}\\t\\t{f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique edge types: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "Unique edge types: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "Unique edge types: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "Training GAT + BatchNorm...\n",
      "Epoch 1, Train Loss: 2.6019, Val Loss: 3.2095\n",
      "Epoch 2, Train Loss: 2.6310, Val Loss: 13.1124\n",
      "Epoch 3, Train Loss: 3.6083, Val Loss: 7.3790\n",
      "Epoch 4, Train Loss: 2.4415, Val Loss: 6.7082\n",
      "Epoch 5, Train Loss: 2.2857, Val Loss: 6.4300\n",
      "Epoch 6, Train Loss: 2.1886, Val Loss: 7.2506\n",
      "Epoch 7, Train Loss: 2.1602, Val Loss: 8.0862\n",
      "Epoch 8, Train Loss: 2.1045, Val Loss: 7.1511\n",
      "Epoch 9, Train Loss: 2.0608, Val Loss: 5.9592\n",
      "Epoch 10, Train Loss: 2.0509, Val Loss: 5.3529\n",
      "Epoch 11, Train Loss: 2.0542, Val Loss: 4.9369\n",
      "Epoch 12, Train Loss: 2.0336, Val Loss: 4.4209\n",
      "Epoch 13, Train Loss: 2.0155, Val Loss: 3.9028\n",
      "Epoch 14, Train Loss: 1.9910, Val Loss: 3.3549\n",
      "Epoch 15, Train Loss: 1.9605, Val Loss: 2.8352\n",
      "Epoch 16, Train Loss: 1.9501, Val Loss: 2.4578\n",
      "Epoch 17, Train Loss: 1.9525, Val Loss: 2.2583\n",
      "Epoch 18, Train Loss: 1.9433, Val Loss: 2.1760\n",
      "Epoch 19, Train Loss: 1.9323, Val Loss: 2.1680\n",
      "Epoch 20, Train Loss: 1.9364, Val Loss: 2.1739\n",
      "Epoch 21, Train Loss: 1.9286, Val Loss: 2.1488\n",
      "Epoch 22, Train Loss: 1.9215, Val Loss: 2.1102\n",
      "Epoch 23, Train Loss: 1.9066, Val Loss: 2.0840\n",
      "Epoch 24, Train Loss: 1.9123, Val Loss: 2.0625\n",
      "Epoch 25, Train Loss: 1.9032, Val Loss: 2.0344\n",
      "Epoch 26, Train Loss: 1.9059, Val Loss: 2.0020\n",
      "Epoch 27, Train Loss: 1.9005, Val Loss: 1.9768\n",
      "Epoch 28, Train Loss: 1.8880, Val Loss: 1.9630\n",
      "Epoch 29, Train Loss: 1.8841, Val Loss: 1.9461\n",
      "Epoch 30, Train Loss: 1.8790, Val Loss: 1.9337\n",
      "Epoch 31, Train Loss: 1.8740, Val Loss: 1.9291\n",
      "Epoch 32, Train Loss: 1.8667, Val Loss: 1.9304\n",
      "Epoch 33, Train Loss: 1.8607, Val Loss: 1.9325\n",
      "Epoch 34, Train Loss: 1.8695, Val Loss: 1.9412\n",
      "Epoch 35, Train Loss: 1.8485, Val Loss: 1.9527\n",
      "Epoch 36, Train Loss: 1.8531, Val Loss: 1.9632\n",
      "Epoch 37, Train Loss: 1.8490, Val Loss: 1.9737\n",
      "Epoch 38, Train Loss: 1.8414, Val Loss: 1.9777\n",
      "Epoch 39, Train Loss: 1.8431, Val Loss: 1.9865\n",
      "Epoch 40, Train Loss: 1.8369, Val Loss: 1.9851\n",
      "Epoch 41, Train Loss: 1.8298, Val Loss: 1.9808\n",
      "Epoch 42, Train Loss: 1.8302, Val Loss: 1.9769\n",
      "Epoch 43, Train Loss: 1.8201, Val Loss: 1.9763\n",
      "Epoch 44, Train Loss: 1.8221, Val Loss: 1.9748\n",
      "Epoch 45, Train Loss: 1.8099, Val Loss: 1.9768\n",
      "Epoch 46, Train Loss: 1.8009, Val Loss: 1.9781\n",
      "Epoch 47, Train Loss: 1.8032, Val Loss: 1.9806\n",
      "Epoch 48, Train Loss: 1.7870, Val Loss: 1.9833\n",
      "Epoch 49, Train Loss: 1.7881, Val Loss: 1.9808\n",
      "Epoch 50, Train Loss: 1.7737, Val Loss: 1.9843\n",
      "Training GCN+GAT+RGCN...\n",
      "Epoch 1, Train Loss: 2.2237, Val Loss: 3.5580\n",
      "Epoch 2, Train Loss: 3.6445, Val Loss: 2.0993\n",
      "Epoch 3, Train Loss: 2.1126, Val Loss: 1.9688\n",
      "Epoch 4, Train Loss: 1.9837, Val Loss: 1.9455\n",
      "Epoch 5, Train Loss: 1.9553, Val Loss: 1.9460\n",
      "Epoch 6, Train Loss: 1.9462, Val Loss: 1.9463\n",
      "Epoch 7, Train Loss: 1.9460, Val Loss: 1.9452\n",
      "Epoch 8, Train Loss: 1.9434, Val Loss: 1.9440\n",
      "Epoch 9, Train Loss: 1.9440, Val Loss: 1.9427\n",
      "Epoch 10, Train Loss: 1.9431, Val Loss: 1.9423\n",
      "Epoch 11, Train Loss: 1.9442, Val Loss: 1.9414\n",
      "Epoch 12, Train Loss: 1.9439, Val Loss: 1.9412\n",
      "Epoch 13, Train Loss: 1.9419, Val Loss: 1.9408\n",
      "Epoch 14, Train Loss: 1.9399, Val Loss: 1.9391\n",
      "Epoch 15, Train Loss: 1.9370, Val Loss: 1.9360\n",
      "Epoch 16, Train Loss: 1.9340, Val Loss: 1.9317\n",
      "Epoch 17, Train Loss: 1.9311, Val Loss: 1.9278\n",
      "Epoch 18, Train Loss: 1.9314, Val Loss: 1.9233\n",
      "Epoch 19, Train Loss: 1.9286, Val Loss: 1.9208\n",
      "Epoch 20, Train Loss: 1.9308, Val Loss: 1.9187\n",
      "Epoch 21, Train Loss: 1.9237, Val Loss: 1.9164\n",
      "Epoch 22, Train Loss: 1.9157, Val Loss: 1.9124\n",
      "Epoch 23, Train Loss: 1.9081, Val Loss: 1.9076\n",
      "Epoch 24, Train Loss: 1.9046, Val Loss: 1.9029\n",
      "Epoch 25, Train Loss: 1.9019, Val Loss: 1.8994\n",
      "Epoch 26, Train Loss: 1.9017, Val Loss: 1.8957\n",
      "Epoch 27, Train Loss: 1.8970, Val Loss: 1.8930\n",
      "Epoch 28, Train Loss: 1.8909, Val Loss: 1.8888\n",
      "Epoch 29, Train Loss: 1.8848, Val Loss: 1.8845\n",
      "Epoch 30, Train Loss: 1.8861, Val Loss: 1.8807\n",
      "Epoch 31, Train Loss: 1.8801, Val Loss: 1.8769\n",
      "Epoch 32, Train Loss: 1.8683, Val Loss: 1.8748\n",
      "Epoch 33, Train Loss: 1.8676, Val Loss: 1.8735\n",
      "Epoch 34, Train Loss: 1.8561, Val Loss: 1.8721\n",
      "Epoch 35, Train Loss: 1.8532, Val Loss: 1.8703\n",
      "Epoch 36, Train Loss: 1.8471, Val Loss: 1.8697\n",
      "Epoch 37, Train Loss: 1.8404, Val Loss: 1.8671\n",
      "Epoch 38, Train Loss: 1.8297, Val Loss: 1.8616\n",
      "Epoch 39, Train Loss: 1.8237, Val Loss: 1.8551\n",
      "Epoch 40, Train Loss: 1.8135, Val Loss: 1.8497\n",
      "Epoch 41, Train Loss: 1.8079, Val Loss: 1.8492\n",
      "Epoch 42, Train Loss: 1.8062, Val Loss: 1.8465\n",
      "Epoch 43, Train Loss: 1.7980, Val Loss: 1.8393\n",
      "Epoch 44, Train Loss: 1.7971, Val Loss: 1.8294\n",
      "Epoch 45, Train Loss: 1.7821, Val Loss: 1.8240\n",
      "Epoch 46, Train Loss: 1.7786, Val Loss: 1.8193\n",
      "Epoch 47, Train Loss: 1.7708, Val Loss: 1.8325\n",
      "Epoch 48, Train Loss: 1.7548, Val Loss: 1.8498\n",
      "Epoch 49, Train Loss: 1.7510, Val Loss: 1.8475\n",
      "Epoch 50, Train Loss: 1.7412, Val Loss: 1.8343\n",
      "Training GCN + Dropout...\n",
      "Epoch 1, Train Loss: 2.3967, Val Loss: 1.9600\n",
      "Epoch 2, Train Loss: 2.1508, Val Loss: 1.9941\n",
      "Epoch 3, Train Loss: 2.0428, Val Loss: 2.0081\n",
      "Epoch 4, Train Loss: 1.9762, Val Loss: 1.9867\n",
      "Epoch 5, Train Loss: 1.9543, Val Loss: 1.9667\n",
      "Epoch 6, Train Loss: 1.9333, Val Loss: 1.9640\n",
      "Epoch 7, Train Loss: 1.9200, Val Loss: 1.9726\n",
      "Epoch 8, Train Loss: 1.9045, Val Loss: 1.9789\n",
      "Epoch 9, Train Loss: 1.8808, Val Loss: 1.9705\n",
      "Epoch 10, Train Loss: 1.8592, Val Loss: 1.9548\n",
      "Epoch 11, Train Loss: 1.8295, Val Loss: 1.9373\n",
      "Epoch 12, Train Loss: 1.8227, Val Loss: 1.9189\n",
      "Epoch 13, Train Loss: 1.8104, Val Loss: 1.9026\n",
      "Epoch 14, Train Loss: 1.7965, Val Loss: 1.8889\n",
      "Epoch 15, Train Loss: 1.7845, Val Loss: 1.8767\n",
      "Epoch 16, Train Loss: 1.7733, Val Loss: 1.8678\n",
      "Epoch 17, Train Loss: 1.7670, Val Loss: 1.8587\n",
      "Epoch 18, Train Loss: 1.7541, Val Loss: 1.8485\n",
      "Epoch 19, Train Loss: 1.7390, Val Loss: 1.8408\n",
      "Epoch 20, Train Loss: 1.7208, Val Loss: 1.8374\n",
      "Epoch 21, Train Loss: 1.7219, Val Loss: 1.8381\n",
      "Epoch 22, Train Loss: 1.7093, Val Loss: 1.8413\n",
      "Epoch 23, Train Loss: 1.6893, Val Loss: 1.8412\n",
      "Epoch 24, Train Loss: 1.6770, Val Loss: 1.8508\n",
      "Epoch 25, Train Loss: 1.6622, Val Loss: 1.8699\n",
      "Epoch 26, Train Loss: 1.6589, Val Loss: 1.8789\n",
      "Epoch 27, Train Loss: 1.6574, Val Loss: 1.8801\n",
      "Epoch 28, Train Loss: 1.6425, Val Loss: 1.8833\n",
      "Epoch 29, Train Loss: 1.6300, Val Loss: 1.8958\n",
      "Epoch 30, Train Loss: 1.6259, Val Loss: 1.9134\n",
      "Epoch 31, Train Loss: 1.6126, Val Loss: 1.9134\n",
      "Epoch 32, Train Loss: 1.6036, Val Loss: 1.9003\n",
      "Epoch 33, Train Loss: 1.5866, Val Loss: 1.9113\n",
      "Epoch 34, Train Loss: 1.5983, Val Loss: 1.9182\n",
      "Epoch 35, Train Loss: 1.5843, Val Loss: 1.9268\n",
      "Epoch 36, Train Loss: 1.5701, Val Loss: 1.9168\n",
      "Epoch 37, Train Loss: 1.5541, Val Loss: 1.9044\n",
      "Epoch 38, Train Loss: 1.5561, Val Loss: 1.8997\n",
      "Epoch 39, Train Loss: 1.5395, Val Loss: 1.9244\n",
      "Epoch 40, Train Loss: 1.5190, Val Loss: 1.9394\n",
      "Epoch 41, Train Loss: 1.5368, Val Loss: 1.9228\n",
      "Epoch 42, Train Loss: 1.5118, Val Loss: 1.9254\n",
      "Epoch 43, Train Loss: 1.5153, Val Loss: 1.9446\n",
      "Epoch 44, Train Loss: 1.5066, Val Loss: 1.9624\n",
      "Epoch 45, Train Loss: 1.4893, Val Loss: 1.9486\n",
      "Epoch 46, Train Loss: 1.4864, Val Loss: 1.9584\n",
      "Epoch 47, Train Loss: 1.4761, Val Loss: 1.9857\n",
      "Epoch 48, Train Loss: 1.4645, Val Loss: 2.0051\n",
      "Epoch 49, Train Loss: 1.4630, Val Loss: 1.9893\n",
      "Epoch 50, Train Loss: 1.4538, Val Loss: 1.9958\n",
      "\n",
      "Benchmark Results:\n",
      "Model\t\t\tTest Accuracy\tTest F1-Score\n",
      "GAT + BatchNorm     \t11.69\t\t10.29\n",
      "GCN+GAT+RGCN        \t22.99\t\t19.97\n",
      "GCN + Dropout       \t27.64\t\t22.27\n"
     ]
    }
   ],
   "source": [
    "main_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Approach 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GraphModel Class Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import RGCNConv, TransformerConv\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, g_dim, h1_dim, h2_dim, num_relations, device, args):\n",
    "        super(GraphModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_modals = args['n_modals']\n",
    "        self.wp = args['wp']\n",
    "        self.wf = args['wf']\n",
    "        self.edge_multi = args['edge_multi']\n",
    "        self.edge_temp = args['edge_temp']\n",
    "\n",
    "        # GNN Layers: RGCN followed by TransformerConv\n",
    "        self.rgcn = RGCNConv(g_dim, h1_dim, num_relations=num_relations)\n",
    "        self.transformer = TransformerConv(h1_dim, h2_dim, heads=4)\n",
    "        self.final_layer = nn.Linear(h2_dim, args['num_classes'])\n",
    "\n",
    "    def forward(self, x, lengths, edge_index, edge_type):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphModel.\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features.\n",
    "            lengths (list): Lengths of sequences in the batch.\n",
    "            edge_index (torch.Tensor): Graph edge indices.\n",
    "            edge_type (torch.Tensor): Graph edge types.\n",
    "        Returns:\n",
    "            torch.Tensor: Final class predictions.\n",
    "        \"\"\"\n",
    "        # Apply RGCN\n",
    "        x = self.rgcn(x, edge_index, edge_type)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Apply TransformerConv\n",
    "        x = self.transformer(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Apply final classification layer\n",
    "        output = self.final_layer(x)\n",
    "        return output\n",
    "\n",
    "    def batch_graphify(self, lengths):\n",
    "        \"\"\"\n",
    "        Constructs the graph structure for the batch.\n",
    "        \"\"\"\n",
    "        edge_indices, edge_types, node_types = [], [], []\n",
    "        total_length = sum(lengths)\n",
    "\n",
    "        start_idx = 0\n",
    "        for length in lengths:\n",
    "            end_idx = start_idx + length\n",
    "\n",
    "            # Node types\n",
    "            node_types += [j % self.n_modals for j in range(start_idx, end_idx)]\n",
    "\n",
    "            # Temporal and multi-modal edges\n",
    "            for i in range(start_idx, end_idx):\n",
    "                for j in range(start_idx, end_idx):\n",
    "                    if i != j:\n",
    "                        if self.edge_temp:\n",
    "                            edge_indices.append([i, j])\n",
    "                            edge_types.append(0 if i < j else 1)  # 0 = future, 1 = past\n",
    "                        if self.edge_multi and node_types[i] != node_types[j]:\n",
    "                            edge_indices.append([i, j])\n",
    "                            edge_types.append(2)  # Multi-modal edge type\n",
    "            start_idx = end_idx\n",
    "\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(self.device)\n",
    "        edge_type = torch.tensor(edge_types, dtype=torch.long).to(self.device)\n",
    "\n",
    "        return edge_index, edge_type\n",
    "\n",
    "    def feature_packing(self, x):\n",
    "        \"\"\"\n",
    "        Packs node features for batch processing.\n",
    "        \"\"\"\n",
    "        return pad_sequence(x, batch_first=True).to(self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, optimizer, criterion, epochs=50, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data.x, train_data.lengths, train_data.edge_index, train_data.edge_type)\n",
    "        loss = criterion(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data.x, val_data.lengths, val_data.edge_index, val_data.edge_type)\n",
    "            val_loss = criterion(val_out, val_data.y)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Test\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_out = model(val_data.x, val_data.lengths, val_data.edge_index, val_data.edge_type)\n",
    "        test_pred = test_out.argmax(dim=1)\n",
    "        test_acc = accuracy_score(val_data.y.cpu(), test_pred.cpu())\n",
    "        test_f1 = f1_score(val_data.y.cpu(), test_pred.cpu(), average='macro')\n",
    "\n",
    "    return test_acc, test_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph_data(data, split, args):\n",
    "    \"\"\"\n",
    "    Prepares data for the GraphModel by extracting features, labels, and constructing the graph structure.\n",
    "    Args:\n",
    "        data (dict): Input data dictionary.\n",
    "        split (str): Dataset split ('train', 'val', 'test').\n",
    "        args (dict): Additional arguments for graph configuration.\n",
    "    Returns:\n",
    "        dict: Prepared graph data including node features, labels, edge indices, and edge types.\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    video_features = torch.mean(data[split]['video']['features'], dim=1)  # Average over temporal dimension\n",
    "    features = torch.cat([\n",
    "        data[split]['audio']['features'],\n",
    "        data[split]['text']['features'],\n",
    "        video_features\n",
    "    ], dim=0)\n",
    "\n",
    "    # Prepare labels\n",
    "    labels = torch.cat([\n",
    "        data[split]['audio']['labels'],\n",
    "        data[split]['text']['labels'],\n",
    "        data[split]['video']['labels']\n",
    "    ], dim=0)\n",
    "\n",
    "    # Calculate lengths for graph batching\n",
    "    lengths = [\n",
    "        data[split]['audio']['features'].size(0),\n",
    "        data[split]['text']['features'].size(0),\n",
    "        video_features.size(0)\n",
    "    ]\n",
    "\n",
    "    # Create edge index and edge types\n",
    "    model = GraphModel(\n",
    "        g_dim=args['g_dim'],\n",
    "        h1_dim=args['h1_dim'],\n",
    "        h2_dim=args['h2_dim'],\n",
    "        num_relations=args['num_relations'],\n",
    "        device=args['device'],\n",
    "        args=args\n",
    "    )\n",
    "    edge_index, edge_type = model.batch_graphify(lengths)\n",
    "\n",
    "    # Return prepared data\n",
    "    return {\n",
    "        'x': features.to(args['device']),\n",
    "        'y': labels.to(args['device']),\n",
    "        'lengths': lengths,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_type': edge_type\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data, args):\n",
    "    # Prepare data\n",
    "    train_data = prepare_graph_data(data, 'train', args)\n",
    "    val_data = prepare_graph_data(data, 'val', args)\n",
    "    test_data = prepare_graph_data(data, 'test', args)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GraphModel(\n",
    "        g_dim=args['g_dim'],\n",
    "        h1_dim=args['h1_dim'],\n",
    "        h2_dim=args['h2_dim'],\n",
    "        num_relations=args['num_relations'],\n",
    "        device=args['device'],\n",
    "        args=args\n",
    "    ).to(args['device'])\n",
    "\n",
    "    # Optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    print(\"Training the GraphModel...\")\n",
    "    acc, f1 = train_and_evaluate(model, train_data, val_data, optimizer, criterion)\n",
    "    print(f\"\\nTest Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"Test F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Arguments and Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data, args):\n",
    "    # Prepare data\n",
    "    try:\n",
    "        train_data = prepare_graph_data(data, 'train', args)\n",
    "        val_data = prepare_graph_data(data, 'val', args)\n",
    "        test_data = prepare_graph_data(data, 'test', args)\n",
    "    except KeyError as e:\n",
    "        print(f\"Data preparation error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize model\n",
    "    model = GraphModel(\n",
    "        g_dim=args['g_dim'],\n",
    "        h1_dim=args['h1_dim'],\n",
    "        h2_dim=args['h2_dim'],\n",
    "        num_relations=args['num_relations'],\n",
    "        device=args['device'],\n",
    "        args=args\n",
    "    ).to(args['device'])\n",
    "\n",
    "    # Optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    print(\"Training the GraphModel...\")\n",
    "    acc, f1 = train_and_evaluate(model, train_data, val_data, optimizer, criterion)\n",
    "    print(f\"\\nTest Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"Test F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'g_dim': 128,  # Input feature dimension\n",
    "    'h1_dim': 64,  # Hidden layer 1 dimension\n",
    "    'h2_dim': 32,  # Hidden layer 2 dimension\n",
    "    'num_classes': len(torch.unique(data['train']['audio']['labels'])),\n",
    "    'num_relations': 3,  # Temporal and multi-modal edges\n",
    "    'n_modals': 3,  # Number of modalities (audio, text, video)\n",
    "    'wp': 2,  # Past window\n",
    "    'wf': 2,  # Future window\n",
    "    'edge_multi': True,  # Enable multi-modal edges\n",
    "    'edge_temp': True,  # Enable temporal edges\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "main_pipeline(data, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
