{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "data_dir = os.path.join('..', \"outputs\", \"embeddings\")\n",
    "with open(os.path.join(data_dir, \"loaders_datasets.pkl\"), 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train audio features shape: torch.Size([9988, 768])\n",
      "train text features shape: torch.Size([9988, 768])\n",
      "train video features shape: torch.Size([9988, 16, 768])\n",
      "train text length tensor shape: torch.Size([9988])\n",
      "val audio features shape: torch.Size([1108, 768])\n",
      "val text features shape: torch.Size([1108, 768])\n",
      "val video features shape: torch.Size([1108, 16, 768])\n",
      "val text length tensor shape: torch.Size([1108])\n",
      "test audio features shape: torch.Size([2610, 768])\n",
      "test text features shape: torch.Size([2610, 768])\n",
      "test video features shape: torch.Size([2610, 16, 768])\n",
      "test text length tensor shape: torch.Size([2610])\n"
     ]
    }
   ],
   "source": [
    "for split in data.keys():\n",
    "    for modal in ['audio', 'text', 'video']:\n",
    "        modal_tensors = data[split][modal].tensors\n",
    "        data[split][modal] = {\n",
    "            'features': modal_tensors[0], \n",
    "            'labels': modal_tensors[1]   \n",
    "        }\n",
    "\n",
    "    if 'text' in data[split]:\n",
    "        text_features = data[split]['text']['features']\n",
    "        text_len_tensor = torch.sum((text_features != 0).long(), dim=1) \n",
    "        data[split]['text']['text_len_tensor'] = text_len_tensor\n",
    "\n",
    "\n",
    "for split in data.keys():\n",
    "    print(f\"{split} audio features shape: {data[split]['audio']['features'].shape}\")\n",
    "    print(f\"{split} text features shape: {data[split]['text']['features'].shape}\")\n",
    "    print(f\"{split} video features shape: {data[split]['video']['features'].shape}\")\n",
    "    print(f\"{split} text length tensor shape: {data[split]['text']['text_len_tensor'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, RGCNConv\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Préparer les données --------\n",
    "def prepare_graph_data(data, split):\n",
    "    # Moyennage sur la dimension temporelle pour la modalité vidéo\n",
    "    video_features = torch.mean(data[split]['video']['features'], dim=1)  # Moyenne sur la dimension temporelle\n",
    "    \n",
    "    # Concaténer les features\n",
    "    features = torch.cat([\n",
    "        data[split]['audio']['features'],\n",
    "        data[split]['text']['features'],\n",
    "        video_features\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Concaténer les labels\n",
    "    labels = torch.cat([\n",
    "        data[split]['audio']['labels'],\n",
    "        data[split]['text']['labels'],\n",
    "        data[split]['video']['labels']\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Initialiser les listes pour edge_index et edge_type\n",
    "    edge_index = []\n",
    "    edge_type = []\n",
    "\n",
    "    # -------- Relations temporelles intra-modales --------\n",
    "    def add_temporal_edges(offset, num_nodes, relation_type):\n",
    "        for i in range(num_nodes):\n",
    "            if i > 0:\n",
    "                edge_index.append([offset + i, offset + i - 1])  # -1 (passé)\n",
    "                edge_type.append(relation_type)\n",
    "            edge_index.append([offset + i, offset + i])  # 0 (présent)\n",
    "            edge_type.append(relation_type + 1)\n",
    "            if i < num_nodes - 1:\n",
    "                edge_index.append([offset + i, offset + i + 1])  # +1 (futur)\n",
    "                edge_type.append(relation_type + 2)\n",
    "\n",
    "    # Ajouter les relations intra-modales\n",
    "    num_audio = data[split]['audio']['features'].size(0)\n",
    "    num_text = data[split]['text']['features'].size(0)\n",
    "    num_video = video_features.size(0)  # Taille après réduction temporelle\n",
    "\n",
    "    add_temporal_edges(0, num_audio, 0)  # Audio : relation types 0, 1, 2\n",
    "    add_temporal_edges(num_audio, num_text, 3)  # Text : relation types 3, 4, 5\n",
    "    add_temporal_edges(num_audio + num_text, num_video, 6)  # Video : relation types 6, 7, 8\n",
    "\n",
    "    # -------- Relations cross-modales --------\n",
    "    for i in range(min(num_audio, num_text, num_video)):\n",
    "        edge_index.append([i, num_audio + num_text + i])  # Audio -> Video\n",
    "        edge_type.append(9)\n",
    "        edge_index.append([num_audio + i, num_audio + num_text + i])  # Text -> Video\n",
    "        edge_type.append(10)\n",
    "        edge_index.append([i, num_audio + i])  # Audio -> Text\n",
    "        edge_type.append(11)\n",
    "\n",
    "    # Convertir en tenseurs\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "\n",
    "    print(\"Unique edge types:\", torch.unique(edge_type))  # Debugging\n",
    "    return Data(x=features, y=labels, edge_index=edge_index, edge_type=edge_type)\n",
    "\n",
    "# -------- Définir les Modèles --------\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.0):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations)\n",
    "        self.conv2 = RGCNConv(hidden_channels, out_channels, num_relations)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# -------- Entraîner et Tester --------\n",
    "def train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(model, RGCN):\n",
    "            out = model(train_data.x, train_data.edge_index, train_data.edge_type)\n",
    "        \n",
    "        else:\n",
    "            out = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "        loss = criterion(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, RGCN):\n",
    "                val_out = model(val_data.x, val_data.edge_index, val_data.edge_type)\n",
    "            else:\n",
    "                val_out = model(val_data.x, val_data.edge_index)\n",
    "            val_loss = criterion(val_out, val_data.y)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, RGCN):\n",
    "            test_out = model(test_data.x, test_data.edge_index, test_data.edge_type)\n",
    "        else:\n",
    "            test_out = model(test_data.x, test_data.edge_index)\n",
    "        test_pred = test_out.argmax(dim=1)\n",
    "        test_acc = accuracy_score(test_data.y.cpu(), test_pred.cpu())\n",
    "        test_f1 = f1_score(test_data.y.cpu(), test_pred.cpu(), average='macro')\n",
    "\n",
    "    return test_acc, test_f1\n",
    "\n",
    "# -------- Pipeline Principal --------\n",
    "def main_pipeline(data):\n",
    "    # Préparer les données\n",
    "    train_data = prepare_graph_data(data, 'train')\n",
    "    val_data = prepare_graph_data(data, 'val')\n",
    "    test_data = prepare_graph_data(data, 'test')\n",
    "\n",
    "    # Configurations de modèles\n",
    "    architectures = [\n",
    "        (\"GCN (2 layers)\", GCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)))),\n",
    "        (\"GAT (4 heads)\", GAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), heads=4)),\n",
    "        (\"RGCN\", RGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=12)),\n",
    "    ]\n",
    "\n",
    "    # Entraîner et évaluer chaque modèle\n",
    "    results = []\n",
    "    for name, model in architectures:\n",
    "        print(f\"Training {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        acc, f1 = train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion)\n",
    "        results.append((name, acc * 100, f1 * 100))\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"Model\\t\\t\\tTest Accuracy\\tTest F1-Score\")\n",
    "    for name, acc, f1 in results:\n",
    "        print(f\"{name:20}\\t{acc:.2f}\\t\\t{f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Improvement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Normalisation des Données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_graph_data(data, split, graph_augmentation=False):\n",
    "    # Normaliser les données\n",
    "    scaler_audio = StandardScaler()\n",
    "    scaler_text = StandardScaler()\n",
    "    scaler_video = StandardScaler()\n",
    "\n",
    "    data[split]['audio']['features'] = torch.tensor(\n",
    "        scaler_audio.fit_transform(data[split]['audio']['features']),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    data[split]['text']['features'] = torch.tensor(\n",
    "        scaler_text.fit_transform(data[split]['text']['features']),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    video_shape = data[split]['video']['features'].shape\n",
    "    data[split]['video']['features'] = torch.tensor(\n",
    "        scaler_video.fit_transform(data[split]['video']['features'].reshape(-1, video_shape[-1])),\n",
    "        dtype=torch.float\n",
    "    ).reshape(video_shape)\n",
    "\n",
    "    # Moyenne sur la dimension temporelle pour la modalité vidéo\n",
    "    video_features = torch.mean(data[split]['video']['features'], dim=1)  # Moyenne sur la dimension temporelle\n",
    "    \n",
    "    # Concaténer les features\n",
    "    features = torch.cat([\n",
    "        data[split]['audio']['features'],\n",
    "        data[split]['text']['features'],\n",
    "        video_features\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Concaténer les labels\n",
    "    labels = torch.cat([\n",
    "        data[split]['audio']['labels'],\n",
    "        data[split]['text']['labels'],\n",
    "        data[split]['video']['labels']\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Initialiser les listes pour edge_index et edge_type\n",
    "    edge_index = []\n",
    "    edge_type = []\n",
    "\n",
    "    # -------- Relations temporelles intra-modales --------\n",
    "    def add_temporal_edges(offset, num_nodes, relation_type):\n",
    "        for i in range(num_nodes):\n",
    "            if i > 1:\n",
    "                edge_index.append([offset + i, offset + i - 2])  # Passé lointain\n",
    "                edge_type.append(relation_type + 3)\n",
    "            if i < num_nodes - 2:\n",
    "                edge_index.append([offset + i, offset + i + 2])  # Futur lointain\n",
    "                edge_type.append(relation_type + 4)\n",
    "\n",
    "            # Passé proche, présent, futur proche\n",
    "            if i > 0:\n",
    "                edge_index.append([offset + i, offset + i - 1])\n",
    "                edge_type.append(relation_type)\n",
    "            edge_index.append([offset + i, offset + i])\n",
    "            edge_type.append(relation_type + 1)\n",
    "            if i < num_nodes - 1:\n",
    "                edge_index.append([offset + i, offset + i + 1])\n",
    "                edge_type.append(relation_type + 2)\n",
    "\n",
    "\n",
    "    # Ajouter les relations intra-modales\n",
    "    num_audio = data[split]['audio']['features'].size(0)\n",
    "    num_text = data[split]['text']['features'].size(0)\n",
    "    num_video = video_features.size(0)  # Taille après réduction temporelle\n",
    "\n",
    "    add_temporal_edges(0, num_audio, 0)  # Audio\n",
    "    add_temporal_edges(num_audio, num_text, 5)  # Text\n",
    "    add_temporal_edges(num_audio + num_text, num_video, 10)  # Video\n",
    "\n",
    "    # Ajouter les relations cross-modales complexes\n",
    "    for i in range(min(num_audio, num_text, num_video)):\n",
    "        edge_index.append([i, num_audio + num_text + i])  # Audio -> Video\n",
    "        edge_type.append(15)\n",
    "        edge_index.append([num_audio + i, num_audio + num_text + i])  # Text -> Video\n",
    "        edge_type.append(16)\n",
    "        edge_index.append([i, num_audio + i])  # Audio -> Text\n",
    "        edge_type.append(17)\n",
    "\n",
    "    # Graph augmentation : ajouter des arêtes aléatoires si activé\n",
    "    if graph_augmentation:\n",
    "        for _ in range(100):  # Par exemple, 100 arêtes aléatoires\n",
    "            src, dst = torch.randint(0, features.size(0), (2,))\n",
    "            edge_index.append([src.item(), dst.item()])\n",
    "            edge_type.append(18)\n",
    "\n",
    "    # Convertir en tenseurs\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "\n",
    "    print(\"Unique edge types:\", torch.unique(edge_type))  # Debugging\n",
    "    return Data(x=features, y=labels, edge_index=edge_index, edge_type=edge_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Equilibrage des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "    weights = 1.0 / counts.float()\n",
    "    weights = weights / weights.sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Augmentation des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_augmentation = True\n",
    "# train_data = prepare_graph_data(data, 'train', graph_augmentation=graph_augmentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Architecture des modèles** (Ajout de Couches Supplémentaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DeepGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.3):\n",
    "        super(DeepGCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.final_layer = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            x = layer(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "class DeepGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, num_layers=3, dropout=0.5):\n",
    "        super(DeepGAT, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GATConv(in_channels, hidden_channels, heads=heads))\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels * heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels * heads))\n",
    "\n",
    "        self.final_layer = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            x = layer(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "class CombinedGCN_GAT_RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations, dropout=0.5):\n",
    "        super(CombinedGCN_GAT_RGCN, self).__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=4)\n",
    "        self.rgcn = RGCNConv(hidden_channels * 4, hidden_channels, num_relations)\n",
    "        self.final_layer = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = self.gcn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rgcn(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class TemporalGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations, heads=4, dropout=0.5):\n",
    "        super(TemporalGAT, self).__init__()\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        self.gat_layers.append(GATConv(in_channels, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        for _ in range(2):  # Exemple : 3 couches de GAT\n",
    "            self.gat_layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        self.final_layer = GATConv(hidden_channels * heads, out_channels, heads=1, add_self_loops=False)\n",
    "        self.num_relations = num_relations\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type=None):\n",
    "        if edge_type is None:\n",
    "            raise ValueError(\"edge_type is required for TemporalGAT\")\n",
    "\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)  \n",
    "            x = gat_layer(x, edge_index)  \n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class TemporalGATv2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.5):\n",
    "        super(TemporalGATv2, self).__init__()\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        self.gat_layers.append(GATv2Conv(in_channels, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        for _ in range(2):\n",
    "            self.gat_layers.append(GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, add_self_loops=False))\n",
    "        self.final_layer = GATv2Conv(hidden_channels * heads, out_channels, heads=1, add_self_loops=False)\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)  # Rename to avoid conflict\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = gat_layer(x, edge_index)\n",
    "\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Relations dans le graphe\n",
    "\n",
    "a) Relations cross-modales\n",
    "\n",
    "Introduisez plus de connexions cross-modales dans edge_index, par exemple en liant chaque nœud audio à plusieurs nœuds vidéo proches au lieu d'un seul.\n",
    "\n",
    "b) Graph Augmentation\n",
    "\n",
    "Ajoutez des bruits aux connexions existantes ou utilisez des méthodes comme DropEdge (supprimer des arêtes aléatoires à chaque itération) pour améliorer la robustesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Autres Modèles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Graph Attention Networks (GATv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedGCN_GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(CombinedGCN_GAT, self).__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=4)\n",
    "        self.final_layer = GCNConv(hidden_channels * 4, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gcn(x, edge_index))\n",
    "        x = F.elu(self.gat(x, edge_index))\n",
    "        x = self.final_layer(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion, epochs=50):\n",
    "    weights = compute_class_weights(train_data.y)\n",
    "    criterion = CrossEntropyLoss(weight=weights)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if hasattr(model, 'rgcn') or isinstance(model, (TemporalGAT, CombinedGCN_GAT_RGCN)):\n",
    "            out = model(train_data.x, train_data.edge_index, train_data.edge_type)\n",
    "        else:\n",
    "            out = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "        loss = criterion(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, (CombinedGCN_GAT_RGCN, TemporalGAT)):\n",
    "                val_out = model(val_data.x, val_data.edge_index, val_data.edge_type)\n",
    "            else:\n",
    "                val_out = model(val_data.x, val_data.edge_index)\n",
    "            val_loss = criterion(val_out, val_data.y)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, (TemporalGAT, CombinedGCN_GAT_RGCN)):\n",
    "            test_out = model(test_data.x, test_data.edge_index, test_data.edge_type)\n",
    "        else:\n",
    "            test_out = model(test_data.x, test_data.edge_index)\n",
    "        test_pred = test_out.argmax(dim=1)\n",
    "        test_acc = accuracy_score(test_data.y.cpu(), test_pred.cpu())\n",
    "        test_f1 = f1_score(test_data.y.cpu(), test_pred.cpu(), average='macro')\n",
    "\n",
    "    return test_acc, test_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data):\n",
    "    train_data = prepare_graph_data(data, 'train', graph_augmentation=False)\n",
    "    val_data = prepare_graph_data(data, 'val', graph_augmentation=False)\n",
    "    test_data = prepare_graph_data(data, 'test', graph_augmentation=False)\n",
    "\n",
    "    # Configurations de modèles\n",
    "    architectures = [\n",
    "        # (\"GCN (3 layers)\", DeepGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3)),\n",
    "        # (\"GCN (5 layers)\", DeepGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=5)),\n",
    "        # (\"GAT (3 layers)\", DeepGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3)),\n",
    "        # (\"GAT (5 layers)\", DeepGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=5)),\n",
    "        # (\"RGCN (5 relations)\", RGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=20)),\n",
    "        # (\"Temporal GAT\", TemporalGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=20)),\n",
    "        # (\"Temporal GATv2\", TemporalGATv2(train_data.x.size(1), 64, len(torch.unique(train_data.y)))),\n",
    "        # (\"Combined GCN+GAT\", CombinedGCN_GAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)))),\n",
    "        (\"GAT + BatchNorm\", DeepGAT(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3)),\n",
    "        (\"GCN+GAT+RGCN\", CombinedGCN_GAT_RGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_relations=20)),\n",
    "        (\"GCN + Dropout\", DeepGCN(train_data.x.size(1), 64, len(torch.unique(train_data.y)), num_layers=3, dropout=0.5)),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, model in architectures:\n",
    "        print(f\"Training {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        acc, f1 = train_and_evaluate(model, train_data, val_data, test_data, optimizer, criterion)\n",
    "        results.append((name, acc * 100, f1 * 100))\n",
    "\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"Model\\t\\t\\tTest Accuracy\\tTest F1-Score\")\n",
    "    for name, acc, f1 in results:\n",
    "        print(f\"{name:20}\\t{acc:.2f}\\t\\t{f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique edge types: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "Unique edge types: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "Unique edge types: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "Training GAT + BatchNorm...\n",
      "Epoch 1, Train Loss: 2.6780, Val Loss: 5.0456\n",
      "Epoch 2, Train Loss: 2.6737, Val Loss: 6.6066\n",
      "Epoch 3, Train Loss: 3.0211, Val Loss: 5.5473\n",
      "Epoch 4, Train Loss: 2.3706, Val Loss: 9.4584\n",
      "Epoch 5, Train Loss: 2.2559, Val Loss: 10.1987\n",
      "Epoch 6, Train Loss: 2.2097, Val Loss: 8.3322\n",
      "Epoch 7, Train Loss: 2.1262, Val Loss: 6.2396\n",
      "Epoch 8, Train Loss: 2.0561, Val Loss: 4.7038\n",
      "Epoch 9, Train Loss: 2.0492, Val Loss: 4.0332\n",
      "Epoch 10, Train Loss: 2.0188, Val Loss: 3.7398\n",
      "Epoch 11, Train Loss: 1.9948, Val Loss: 3.4640\n",
      "Epoch 12, Train Loss: 1.9643, Val Loss: 3.1707\n",
      "Epoch 13, Train Loss: 1.9704, Val Loss: 2.8102\n",
      "Epoch 14, Train Loss: 1.9397, Val Loss: 2.4389\n",
      "Epoch 15, Train Loss: 1.9419, Val Loss: 2.2390\n",
      "Epoch 16, Train Loss: 1.9340, Val Loss: 2.1728\n",
      "Epoch 17, Train Loss: 1.9289, Val Loss: 2.1286\n",
      "Epoch 18, Train Loss: 1.9226, Val Loss: 2.0925\n",
      "Epoch 19, Train Loss: 1.9184, Val Loss: 2.0500\n",
      "Epoch 20, Train Loss: 1.9138, Val Loss: 2.0060\n",
      "Epoch 21, Train Loss: 1.9119, Val Loss: 1.9766\n",
      "Epoch 22, Train Loss: 1.9073, Val Loss: 1.9631\n",
      "Epoch 23, Train Loss: 1.8930, Val Loss: 1.9545\n",
      "Epoch 24, Train Loss: 1.8883, Val Loss: 1.9474\n",
      "Epoch 25, Train Loss: 1.8814, Val Loss: 1.9456\n",
      "Epoch 26, Train Loss: 1.8759, Val Loss: 1.9489\n",
      "Epoch 27, Train Loss: 1.8627, Val Loss: 1.9529\n",
      "Epoch 28, Train Loss: 1.8554, Val Loss: 1.9559\n",
      "Epoch 29, Train Loss: 1.8612, Val Loss: 1.9572\n",
      "Epoch 30, Train Loss: 1.8485, Val Loss: 1.9587\n",
      "Epoch 31, Train Loss: 1.8428, Val Loss: 1.9571\n",
      "Epoch 32, Train Loss: 1.8377, Val Loss: 1.9554\n",
      "Epoch 33, Train Loss: 1.8337, Val Loss: 1.9549\n",
      "Epoch 34, Train Loss: 1.8245, Val Loss: 1.9565\n",
      "Epoch 35, Train Loss: 1.8099, Val Loss: 1.9635\n",
      "Epoch 36, Train Loss: 1.8114, Val Loss: 1.9689\n",
      "Epoch 37, Train Loss: 1.8003, Val Loss: 1.9702\n",
      "Epoch 38, Train Loss: 1.8037, Val Loss: 1.9695\n",
      "Epoch 39, Train Loss: 1.7897, Val Loss: 1.9641\n",
      "Epoch 40, Train Loss: 1.7755, Val Loss: 1.9567\n",
      "Epoch 41, Train Loss: 1.7803, Val Loss: 1.9554\n",
      "Epoch 42, Train Loss: 1.7639, Val Loss: 1.9544\n",
      "Epoch 43, Train Loss: 1.7577, Val Loss: 1.9588\n",
      "Epoch 44, Train Loss: 1.7494, Val Loss: 1.9670\n",
      "Epoch 45, Train Loss: 1.7486, Val Loss: 1.9774\n",
      "Epoch 46, Train Loss: 1.7509, Val Loss: 1.9830\n",
      "Epoch 47, Train Loss: 1.7369, Val Loss: 1.9848\n",
      "Epoch 48, Train Loss: 1.7174, Val Loss: 1.9793\n",
      "Epoch 49, Train Loss: 1.7106, Val Loss: 1.9838\n",
      "Epoch 50, Train Loss: 1.7057, Val Loss: 1.9846\n",
      "Training GCN+GAT+RGCN...\n",
      "Epoch 1, Train Loss: 2.0845, Val Loss: 3.8033\n",
      "Epoch 2, Train Loss: 4.5153, Val Loss: 2.0061\n",
      "Epoch 3, Train Loss: 2.0671, Val Loss: 1.9692\n",
      "Epoch 4, Train Loss: 2.0018, Val Loss: 1.9550\n",
      "Epoch 5, Train Loss: 1.9703, Val Loss: 1.9520\n",
      "Epoch 6, Train Loss: 1.9530, Val Loss: 1.9486\n",
      "Epoch 7, Train Loss: 1.9506, Val Loss: 1.9466\n",
      "Epoch 8, Train Loss: 1.9463, Val Loss: 1.9458\n",
      "Epoch 9, Train Loss: 1.9462, Val Loss: 1.9453\n",
      "Epoch 10, Train Loss: 1.9457, Val Loss: 1.9449\n",
      "Epoch 11, Train Loss: 1.9446, Val Loss: 1.9439\n",
      "Epoch 12, Train Loss: 1.9441, Val Loss: 1.9431\n",
      "Epoch 13, Train Loss: 1.9446, Val Loss: 1.9431\n",
      "Epoch 14, Train Loss: 1.9441, Val Loss: 1.9432\n",
      "Epoch 15, Train Loss: 1.9429, Val Loss: 1.9429\n",
      "Epoch 16, Train Loss: 1.9432, Val Loss: 1.9421\n",
      "Epoch 17, Train Loss: 1.9416, Val Loss: 1.9413\n",
      "Epoch 18, Train Loss: 1.9421, Val Loss: 1.9410\n",
      "Epoch 19, Train Loss: 1.9402, Val Loss: 1.9405\n",
      "Epoch 20, Train Loss: 1.9405, Val Loss: 1.9393\n",
      "Epoch 21, Train Loss: 1.9401, Val Loss: 1.9379\n",
      "Epoch 22, Train Loss: 1.9398, Val Loss: 1.9365\n",
      "Epoch 23, Train Loss: 1.9357, Val Loss: 1.9337\n",
      "Epoch 24, Train Loss: 1.9343, Val Loss: 1.9321\n",
      "Epoch 25, Train Loss: 1.9345, Val Loss: 1.9306\n",
      "Epoch 26, Train Loss: 1.9321, Val Loss: 1.9305\n",
      "Epoch 27, Train Loss: 1.9301, Val Loss: 1.9291\n",
      "Epoch 28, Train Loss: 1.9308, Val Loss: 1.9253\n",
      "Epoch 29, Train Loss: 1.9232, Val Loss: 1.9201\n",
      "Epoch 30, Train Loss: 1.9228, Val Loss: 1.9158\n",
      "Epoch 31, Train Loss: 1.9194, Val Loss: 1.9158\n",
      "Epoch 32, Train Loss: 1.9159, Val Loss: 1.9171\n",
      "Epoch 33, Train Loss: 1.9168, Val Loss: 1.9180\n",
      "Epoch 34, Train Loss: 1.9073, Val Loss: 1.9101\n",
      "Epoch 35, Train Loss: 1.9098, Val Loss: 1.9059\n",
      "Epoch 36, Train Loss: 1.8987, Val Loss: 1.9053\n",
      "Epoch 37, Train Loss: 1.8994, Val Loss: 1.9071\n",
      "Epoch 38, Train Loss: 1.8967, Val Loss: 1.9001\n",
      "Epoch 39, Train Loss: 1.8851, Val Loss: 1.8932\n",
      "Epoch 40, Train Loss: 1.8841, Val Loss: 1.8959\n",
      "Epoch 41, Train Loss: 1.8816, Val Loss: 1.8942\n",
      "Epoch 42, Train Loss: 1.8716, Val Loss: 1.8858\n",
      "Epoch 43, Train Loss: 1.8690, Val Loss: 1.8815\n",
      "Epoch 44, Train Loss: 1.8626, Val Loss: 1.8776\n",
      "Epoch 45, Train Loss: 1.8568, Val Loss: 1.8734\n",
      "Epoch 46, Train Loss: 1.8540, Val Loss: 1.8682\n",
      "Epoch 47, Train Loss: 1.8500, Val Loss: 1.8716\n",
      "Epoch 48, Train Loss: 1.8428, Val Loss: 1.8810\n",
      "Epoch 49, Train Loss: 1.8402, Val Loss: 1.8710\n",
      "Epoch 50, Train Loss: 1.8347, Val Loss: 1.8658\n",
      "Training GCN + Dropout...\n",
      "Epoch 1, Train Loss: 2.3222, Val Loss: 1.9536\n",
      "Epoch 2, Train Loss: 2.1127, Val Loss: 1.9710\n",
      "Epoch 3, Train Loss: 2.0164, Val Loss: 1.9609\n",
      "Epoch 4, Train Loss: 1.9606, Val Loss: 1.9492\n",
      "Epoch 5, Train Loss: 1.9381, Val Loss: 1.9359\n",
      "Epoch 6, Train Loss: 1.9085, Val Loss: 1.9248\n",
      "Epoch 7, Train Loss: 1.8962, Val Loss: 1.9205\n",
      "Epoch 8, Train Loss: 1.8799, Val Loss: 1.9158\n",
      "Epoch 9, Train Loss: 1.8640, Val Loss: 1.9064\n",
      "Epoch 10, Train Loss: 1.8478, Val Loss: 1.8930\n",
      "Epoch 11, Train Loss: 1.8314, Val Loss: 1.8834\n",
      "Epoch 12, Train Loss: 1.8075, Val Loss: 1.8759\n",
      "Epoch 13, Train Loss: 1.7953, Val Loss: 1.8700\n",
      "Epoch 14, Train Loss: 1.7786, Val Loss: 1.8634\n",
      "Epoch 15, Train Loss: 1.7578, Val Loss: 1.8573\n",
      "Epoch 16, Train Loss: 1.7414, Val Loss: 1.8487\n",
      "Epoch 17, Train Loss: 1.7294, Val Loss: 1.8384\n",
      "Epoch 18, Train Loss: 1.7247, Val Loss: 1.8306\n",
      "Epoch 19, Train Loss: 1.7115, Val Loss: 1.8290\n",
      "Epoch 20, Train Loss: 1.7096, Val Loss: 1.8328\n",
      "Epoch 21, Train Loss: 1.6897, Val Loss: 1.8351\n",
      "Epoch 22, Train Loss: 1.6752, Val Loss: 1.8403\n",
      "Epoch 23, Train Loss: 1.6600, Val Loss: 1.8464\n",
      "Epoch 24, Train Loss: 1.6573, Val Loss: 1.8528\n",
      "Epoch 25, Train Loss: 1.6514, Val Loss: 1.8649\n",
      "Epoch 26, Train Loss: 1.6394, Val Loss: 1.8825\n",
      "Epoch 27, Train Loss: 1.6311, Val Loss: 1.8904\n",
      "Epoch 28, Train Loss: 1.6158, Val Loss: 1.8901\n",
      "Epoch 29, Train Loss: 1.6115, Val Loss: 1.9060\n",
      "Epoch 30, Train Loss: 1.6029, Val Loss: 1.9199\n",
      "Epoch 31, Train Loss: 1.5964, Val Loss: 1.9324\n",
      "Epoch 32, Train Loss: 1.5908, Val Loss: 1.9340\n",
      "Epoch 33, Train Loss: 1.5686, Val Loss: 1.9338\n",
      "Epoch 34, Train Loss: 1.5617, Val Loss: 1.9532\n",
      "Epoch 35, Train Loss: 1.5641, Val Loss: 1.9708\n",
      "Epoch 36, Train Loss: 1.5422, Val Loss: 1.9609\n",
      "Epoch 37, Train Loss: 1.5319, Val Loss: 1.9514\n",
      "Epoch 38, Train Loss: 1.5267, Val Loss: 1.9640\n",
      "Epoch 39, Train Loss: 1.5177, Val Loss: 1.9934\n",
      "Epoch 40, Train Loss: 1.5124, Val Loss: 2.0109\n",
      "Epoch 41, Train Loss: 1.5066, Val Loss: 2.0118\n",
      "Epoch 42, Train Loss: 1.4839, Val Loss: 1.9733\n",
      "Epoch 43, Train Loss: 1.4803, Val Loss: 1.9625\n",
      "Epoch 44, Train Loss: 1.4740, Val Loss: 1.9691\n",
      "Epoch 45, Train Loss: 1.4637, Val Loss: 1.9810\n",
      "Epoch 46, Train Loss: 1.4590, Val Loss: 1.9960\n",
      "Epoch 47, Train Loss: 1.4488, Val Loss: 1.9981\n",
      "Epoch 48, Train Loss: 1.4393, Val Loss: 2.0043\n",
      "Epoch 49, Train Loss: 1.4402, Val Loss: 2.0302\n",
      "Epoch 50, Train Loss: 1.4252, Val Loss: 2.0381\n",
      "\n",
      "Benchmark Results:\n",
      "Model\t\t\tTest Accuracy\tTest F1-Score\n",
      "GAT + BatchNorm     \t16.35\t\t14.62\n",
      "GCN+GAT+RGCN        \t18.62\t\t15.50\n",
      "GCN + Dropout       \t26.51\t\t21.91\n"
     ]
    }
   ],
   "source": [
    "main_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
